---
# test
- name: Read podman password from file
  set_fact:
    podman_password: "{{ cro_podman_password }}"
 #   podman_password1: "{{ lookup('file', 'podman_pass') }}"

- name: Set OperatorHub disableAllDefaultSources is to true
  kubernetes.core.k8s:
    definition:
      apiVersion: config.openshift.io/v1
      kind: OperatorHub
      metadata:
        name: cluster
      spec:
        disableAllDefaultSources: true
    state: patched
  register: output

- name: "Fail if unable to patch the disableAllDefaultSources: true"
  fail:
    msg: "Unable to set disableAllDefaultSources to true : {{  output.stderr }}"
  when: output.failed

- name: Get pull secret and decode
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Secret
    namespace: openshift-config
    name: pull-secret
  register: pull_secret_result

- name: storing info
  set_fact:
    data_pull: "{{ pull_secret_result.resources[0].data }}"

- name: Save decoded pull secret to file
  copy:
    content: "{{ pull_secret_result.resources[0].data['.dockerconfigjson'] | b64decode }}"
    dest: /root/authfile

#--------------------needed to test this task before integrating into main branch--
#- name: Log in to podman
#  community.general.podman_login:
#    authfile: ./authfile
#    username: "|shared-qe-temp.src5.75b4d5"
#    password: "{{ podman_password }}"
#---------------------------------

# Podman login
- name: Podman Login
  shell: |
    podman login --authfile /root/authfile --username "{{ cro_podman_username }}"  --password "{{ cro_podman_password }}"
  register: output
  ignore_errors: yes

- name: Fail if podman login failed
  fail:
    msg: "Podman login failed : {{  output.stderr }}"
  when: output.failed

# Setting pullsecret
- name: Set data/pullsecret
  shell: |
    oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=/root/authfile
  register: output
  ignore_errors: yes

- name: Fail if Set data/pullsecret failed
  fail:
    msg: "Set data/pullsecret failed : {{  output.stderr }}"
  when: output.failed

- name: Create ImageContentSourcePolicy
  kubernetes.core.k8s:
    definition:
      apiVersion: operator.openshift.io/v1alpha1
      kind: ImageContentSourcePolicy
      metadata:
        name: brew-registry
      spec:
        repositoryDigestMirrors:
          - mirrors:
              - brew.registry.redhat.io
            source: registry.redhat.io
          - mirrors:
              - brew.registry.redhat.io
            source: registry.stage.redhat.io
          - mirrors:
              - brew.registry.redhat.io
            source: registry-proxy.engineering.redhat.com
    state: present

- name: Create CatalogSource
  kubernetes.core.k8s:
    definition:
      apiVersion: operators.coreos.com/v1alpha1
      kind: CatalogSource
      metadata:
        name: redhat-operators-stage
        namespace: openshift-marketplace
      spec:
        sourceType: grpc
        publisher: redhat
        displayName: "Red Hat Operators CRO"
        image: "{{ cro_catalogsource_image }}"
    state: present

- block:
  - name: "Wait for redhat-operators-stage CatalogSource to be ready"
    kubernetes.core.k8s_info:
      kind: pod
      namespace: openshift-marketplace
      label_selectors:
        - "olm.catalogSource=redhat-operators-stage"
    register: pod
    until: (pod.resources | length == 1) and (pod.resources[0].status.containerStatuses is defined) and (pod.resources[0].status.containerStatuses | map(attribute='ready') | unique == [true])
    retries: 30
    delay: 10
  rescue:
    - name: "Failure"
      fail:
        msg: "Failed waiting for catalog pod to be ready."

- name: Include role to create image content source policy and catalog source
  include_role:
    name: set-custom-catalogsource
  vars:
    custom_catalogsource_name: "{{ cro_catalogsource }}"
    custom_catalogsource_display_name: "CRO Operator catalog source"
    custom_catalogsource_image: "{{ cro_catalogsource_image }}"
  when: cro_catalogsource_image != '' and cro_catalogsource_image != None

- name: Create Namespace
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        labels:
          pod-security.kubernetes.io/audit: privileged
          pod-security.kubernetes.io/enforce: privileged
          pod-security.kubernetes.io/warn: privileged
          security.openshift.io/scc.podSecurityLabelSync: "false"
          kubernetes.io/metadata.name: clusterresourceoverride-operator
        name: clusterresourceoverride-operator
      name: clusterresourceoverride-operator
      spec:
        finalizers:
          - kubernetes
    state: present

- name: Create OperatorGroup
  kubernetes.core.k8s:
    definition:
      apiVersion: operators.coreos.com/v1
      kind: OperatorGroup
      metadata:
        generateName: clusterresourceoverride-
        name: clusterresourceoverride-operator
        namespace: clusterresourceoverride-operator
      spec:
        targetNamespaces:
          - clusterresourceoverride-operator
    state: present

- name: Create Subscription
  kubernetes.core.k8s:
    definition:
      apiVersion: operators.coreos.com/v1alpha1
      kind: Subscription
      metadata:
        name: clusterresourceoverride
        namespace: clusterresourceoverride-operator
      spec:
        channel: stable
        installPlanApproval: Automatic
        name: clusterresourceoverride
        source: redhat-operators-stage
        sourceNamespace: openshift-marketplace
    state: present

- block:
  - name: Wait for Operator pod to be ready
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Pod
      namespace: clusterresourceoverride-operator
      field_selectors:
        - "spec.serviceAccountName=clusterresourceoverride-operator"
    register: pod
    until: (pod.resources | length == 1) and (pod.resources[0].status.containerStatuses is defined) and (pod.resources[0].status.containerStatuses | map(attribute='ready') | unique == [true])
    retries: 10
    delay: 60
  rescue:
    - name: "Failure"
      fail:
        msg: "Operator pod not up after 10 mins"
#Login to the console with user: kubeadmin and password: Jd9JR-Coo4y-ALapc-6LW3z
#- name: Check if Operator pod is ready
#  debug:
#    msg: "Operator pod is up and running"
#  when: operator_pod_info.resources | selectattr('status.phase', 'eq', 'Running') | list | length == 1
#
#- name: Check if Operator pod is not ready after 10 mins
#  fail:
#    msg: "Operator pod not up after 10 mins"
#  when: operator_pod_info.resources | selectattr('status.phase', 'eq', 'Running') | list | length == 0

- name: Create ClusterResourceOverride
  kubernetes.core.k8s:
    definition:
      apiVersion: operator.autoscaling.openshift.io/v1
      kind: ClusterResourceOverride
      metadata:
        name: cluster
      spec:
        podResourceOverride:
          spec:
            memoryRequestToLimitPercent: 50
            cpuRequestToLimitPercent: 25
            limitCPUToMemoryPercent: 200
    state: present

- name: Download And extracting go
  unarchive:
    src: "{{ cro_golang_tarball }}"
    dest: /usr/local
    remote_src: yes
    creates: /usr/local/go

- name: Clone cluster-resource-override-admission-operator repository
  git:
    repo: "https://github.com/openshift/cluster-resource-override-admission-operator.git"
    dest: "/root/cluster-resource-override-admission-operator"  

- name: Build cluster resource override admission operator
  shell: |
      set -e
      export GOPATH=/usr/local/go
      export PATH=$PATH:$GOPATH/bin
      cd /root/cluster-resource-override-admission-operator
      make e2e OPERATOR_NAMESPACE=clusterresourceoverride-operator KUBECONFIG=/root/openstack-upi/auth/kubeconfig 2>&1 | tee /root/cro_e2e_output.txt
  args:
    chdir: /root/cluster-resource-override-admission-operator

- block:
    # Delete CRO subscription
    - name: "Delete CRO subscription"
      kubernetes.core.k8s:
        kind: "Subscription"
        namespace: "clusterresourceoverride-operator"
        name: "clusterresourceoverride-operator"
        api_version: '*'
        state: absent
    # Delete the CRO limits
    - name: "Delete the CRO limits"
      kubernetes.core.k8s:
        api_version: operator.autoscaling.openshift.io/v1
        kind: ClusterResourceOverride
        name: cluster
        state: absent
    # Delete the Operator Group
    - name: "Delete the Operator Group"
      kubernetes.core.k8s:
        api_version: operators.coreos.com/v1
        kind: OperatorGroup
        name: clusterresourceoverride-operator
        state: absent
    # Delete the Namespace
    - name: "Delete the Namespace"
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: clusterresourceoverride-operator
        state: absent
    # Delete Redhat Operator CatalogSource
    - name: "Delete Redhat Operator CatalogSource"
      kubernetes.core.k8s:
        api_version: operators.coreos.com/v1alpha1
        kind: CatalogSource
        name: redhat-operators-stage
        namespace: openshift-marketplace
        state: absent
    # Delete ImageContentSourcePolicy
    - name: "Delete ImageContentSourcePolicy"
      kubernetes.core.k8s:
        api_version: operator.openshift.io/v1alpha1
        kind: ImageContentSourcePolicy
        name: "brew-registry"
        state: absent
  rescue:
    - name: "Failure"
      fail:
        msg: "failed to delete the resources"